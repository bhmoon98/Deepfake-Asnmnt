{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DHSMpeFLMPTV"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd  drive/MyDrive/Colab Notebooks/'Deepfake Asnmnt'/"],"metadata":{"id":"NYWu4SJ_MeKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install timm"],"metadata":{"id":"deSal94BVhai"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import timm \n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.transforms import transforms\n","from PIL import Image\n","from sklearn.metrics import precision_recall_fscore_support"],"metadata":{"id":"_TbdxpHFNHSS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"0QZZuaDgMn7u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# num_classes = 2\n","# xc_model = timm.create_model('xception', pretrained = True, num_classes = num_classes).to(device)\n","# ef_model = timm.create_model('efficientnet_b0', pretrained=True, num_classes = num_classes).to(device)\n","\n","# criterion = nn.BCEWithLogitsLoss()\n","# optimizer_xc = optim.Adam(xc_model.parameters(), lr=0.001)\n","# optimizer_ef = optim.Adam(ef_model.parameters(), lr=0.001)\n","\n","# x     = torch.randn(32, 3, 224, 224).to(device) # 32 is batch_size\n","# ef_model(x).shape"],"metadata":{"id":"mZ6nOu1lWW8V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BaseDataset(Dataset):\n","    def __init__(self, data_path):\n","        self.data_path = data_path\n","\n","        # normalize\n","        self.transforms = transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","        \n","        self.imgs = []\n","        self.labels = []\n","        \n","        # parsing and getting labels\n","        for file in os.listdir(data_path):\n","            if file.endswith('.png'):\n","                img_path = os.path.join(data_path, file)\n","                label = file.split('_')[0]\n","                self.imgs.append(img_path)\n","                self.labels.append(label)\n","        \n","    def __len__(self):\n","        return len(self.imgs)\n","    \n","    def __getitem__(self, idx):\n","        img = Image.open(self.imgs[idx])\n","        img = self.transforms(img)\n","        label = 1 if self.labels[idx] == 'fake' else 0\n","        return img, label"],"metadata":{"id":"Nek81guOijFo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Experiment Class\n","class Experiment:\n","    def __init__(self, title):\n","        self.num_classes = 2\n","        self.title = title\n","        self.xc_model = timm.create_model('xception', pretrained = True, num_classes = self.num_classes).to(device)\n","        self.ef_model = timm.create_model('efficientnet_b0', pretrained=True, num_classes = self.num_classes).to(device)\n","\n","        self.criterion = nn.BCEWithLogitsLoss()\n","        self.optimizer_xc = optim.Adam(self.xc_model.parameters(), lr=0.001, weight_decay = 0.001)\n","        self.optimizer_ef = optim.Adam(self.ef_model.parameters(), lr=0.001, weight_decay = 0.001)\n","\n","\n","    def train_model(self, trainloader, valloader):\n","        num_epochs = 100\n","\n","        xc_train_accs = []\n","        xc_val_accs = []\n","        ef_train_accs = []\n","        ef_val_accs = []\n","\n","        for epoch in range(num_epochs):\n","            # Training loop for Xception model\n","            self.xc_model.train()\n","            xc_total_loss = 0\n","            xc_total_correct = 0\n","            for images, labels in trainloader:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                self.optimizer_xc.zero_grad()\n","                outputs = self.xc_model(images)\n","                labels_onehot = F.one_hot(labels, num_classes = self.num_classes).float()\n","                loss = self.criterion(outputs, labels_onehot)\n","                loss.backward()\n","                self.optimizer_xc.step()\n","\n","                xc_total_loss += loss.item()\n","                xc_total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","\n","            xc_train_loss = xc_total_loss / len(trainloader)\n","            xc_train_acc = xc_total_correct / len(trainloader.dataset)\n","            xc_train_accs.append(xc_train_acc)\n","\n","            print(f'Epoch {epoch+1} - Xception model - Training loss: {xc_train_loss:.4f} - Training accuracy: {xc_train_acc:.4f}')\n","\n","            # Training loop for EfficientNet model\n","            self.ef_model.train()\n","            ef_total_loss = 0\n","            ef_total_correct = 0\n","            for images, labels in trainloader:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                self.optimizer_ef.zero_grad()\n","                outputs = self.ef_model(images)\n","                labels_onehot = F.one_hot(labels, num_classes=self.num_classes).float()\n","                loss = self.criterion(outputs, labels_onehot)\n","                loss.backward()\n","                self.optimizer_ef.step()\n","\n","                ef_total_loss += loss.item()\n","                ef_total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","\n","            ef_train_loss = ef_total_loss / len(trainloader)\n","            ef_train_acc = ef_total_correct / len(trainloader.dataset)\n","            ef_train_accs.append(ef_train_acc)\n","\n","            print(f'Epoch {epoch+1} - EfficientNet model - Training loss: {ef_train_loss:.4f} - Training accuracy: {ef_train_acc:.4f}')\n","\n","\n","            # Validation loop\n","            self.xc_model.eval()\n","            self.ef_model.eval()\n","            with torch.no_grad():\n","                xc_total, xc_correct = 0, 0\n","                for images, labels in valloader:\n","                    images = images.to(device)\n","                    labels = labels.to(device)\n","                    outputs = self.xc_model(images)\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    xc_total += labels.size(0)\n","                    xc_correct += (predicted == labels).sum().item()\n","                xc_accuracy = xc_correct / xc_total\n","                xc_val_accs.append(xc_accuracy)\n","            \n","                print(f'Epoch {epoch+1} - Xception model - Validation accuracy: {xc_accuracy:.4f}')\n","\n","                ef_total, ef_correct = 0, 0\n","                for images, labels in valloader:\n","                    images = images.to(device)\n","                    labels = labels.to(device)\n","                    outputs = self.ef_model(images)\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    ef_total += labels.size(0)\n","                    ef_correct += (predicted == labels).sum().item()\n","                ef_accuracy = ef_correct / ef_total\n","                ef_val_accs.append(ef_accuracy)\n","\n","                print(f'Epoch {epoch+1} - EfficientNet model - Validation accuracy: {ef_accuracy:.4f}')\n","\n","        plt.plot(xc_train_accs, label='Xception Training Accuracy')\n","        plt.plot(xc_val_accs, label='Xception Validation Accuracy')\n","        plt.plot(ef_train_accs, label='EfficientNet Training Accuracy')\n","        plt.plot(ef_val_accs, label='EfficientNet Validation Accuracy')\n","        plt.legend()\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Accuracy')\n","        plt.title(self.title)\n","        plt.show()\n","        plt.savefig(f'{self.title} Train, Validation Accuracy.png')\n","\n","    def test_model(self, testloader):\n","\n","        self.xc_model.eval()\n","        self.ef_model.eval()\n","        with torch.no_grad():\n","            xc_total, xc_correct = 0, 0\n","            xc_true_labels, xc_predicted_labels = [], []\n","            for images, labels in testloader:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                outputs = self.xc_model(images)\n","                _, predicted = torch.max(outputs.data, 1)\n","                xc_total += labels.size(0)\n","                xc_correct += (predicted == labels).sum().item()\n","                xc_true_labels += labels.cpu().tolist()\n","                xc_predicted_labels += predicted.cpu().tolist()\n","            xc_accuracy = xc_correct / xc_total\n","            xc_precision, xc_recall, xc_f1, _ = precision_recall_fscore_support(xc_true_labels, xc_predicted_labels, average='binary')\n","        \n","            print(f'Xception model - Test accuracy: {xc_accuracy:.4f}')\n","            print(f'Xception model - Test precision: {xc_precision:.4f}')\n","            print(f'Xception model - Test recall: {xc_recall:.4f}')\n","            print(f'Xception model - Test F1 score: {xc_f1:.4f}')\n","\n","\n","            ef_total, ef_correct = 0, 0\n","            ef_true_labels, ef_predicted_labels = [], []\n","            for images, labels in testloader:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                outputs = self.ef_model(images)\n","                _, predicted = torch.max(outputs.data, 1)\n","                ef_total += labels.size(0)\n","                ef_correct += (predicted == labels).sum().item()\n","                ef_true_labels += labels.cpu().tolist()\n","                ef_predicted_labels += predicted.cpu().tolist()\n","            ef_accuracy = ef_correct / ef_total\n","            ef_precision, ef_recall, ef_f1, _ = precision_recall_fscore_support(ef_true_labels, ef_predicted_labels, average='binary')\n","\n","            print(f'EfficientNet model - Test accuracy: {ef_accuracy:.4f}')\n","            print(f'EfficientNet model - Test precision: {ef_precision:.4f}')\n","            print(f'EfficientNet model - Test recall: {ef_recall:.4f}')\n","            print(f'EfficientNet model - Test F1 score: {ef_f1:.4f}')\n","\n","            # create histogram\n","            xc_metrics = [xc_accuracy, xc_precision, xc_recall, xc_f1]\n","            ef_metrics = [ef_accuracy, ef_precision, ef_recall, ef_f1]\n","\n","            metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n","\n","            bar_width = 0.35\n","            plt.bar(np.arange(len(metric_names)), xc_metrics, label='Xception', color='blue', width=bar_width)\n","            plt.bar(np.arange(len(metric_names)) + bar_width, ef_metrics, label='EfficientNet', color='green', width=bar_width)\n","\n","            plt.xlabel('Metric')\n","            plt.ylabel('Value')\n","            plt.title(self.title)\n","            plt.xticks(np.arange(len(metric_names)) + bar_width/2, metric_names)\n","            plt.legend()\n","            plt.show()\n","            plt.savefig(f'{self.title} Test Result.png')"],"metadata":{"id":"V-bM1v7FhGbS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#1) HighQuality-Face2Face\n","\n","hf_train_dataset = BaseDataset(data_path = \"High Quality/f2f_data/train\")\n","hf_trainloader = DataLoader(hf_train_dataset, batch_size=32, shuffle=True)\n","hf_val_dataset = BaseDataset(data_path = \"High Quality/f2f_data/val\")\n","hf_valloader = DataLoader(hf_val_dataset, batch_size=32, shuffle=True)\n","hf_test_dataset = BaseDataset(data_path = \"High Quality/f2f_data/test\")\n","hf_testloader = DataLoader(hf_test_dataset, batch_size=32, shuffle=True)\n","\n","hf = Experiment(\"HighQuality-Face2Face\")\n","# hf.train_model(hf_trainloader, hf_valloader)\n","hf.test_model(hf_testloader)"],"metadata":{"id":"ZEka6VQlWX__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2) HighQuality-NeuralTexture\n","\n","hn_train_dataset = BaseDataset(data_path = \"High Quality/nt_data/train\")\n","hn_trainloader = DataLoader(hn_train_dataset, batch_size=32, shuffle=True)\n","hn_val_dataset = BaseDataset(data_path = \"High Quality/nt_data/val\")\n","hn_valloader = DataLoader(hn_val_dataset, batch_size=32, shuffle=True)\n","hn_test_dataset = BaseDataset(data_path = \"High Quality/nt_data/test\")\n","hn_testloader = DataLoader(hn_test_dataset, batch_size=32, shuffle=True)\n","\n","hn = Experiment(\"HighQuality-NeuralTexture\")\n","hn.train_model(hn_trainloader, hn_valloader)\n","hn.test_model(hn_testloader)"],"metadata":{"id":"CdBRsk3R9wQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3) LowQuality-Face2Face\n","\n","lf_train_dataset = BaseDataset(data_path = \"Low Quality/f2f_data/train\")\n","lf_trainloader = DataLoader(lf_train_dataset, batch_size=32, shuffle=True)\n","lf_val_dataset = BaseDataset(data_path = \"Low Quality/f2f_data/val\")\n","lf_valloader = DataLoader(lf_val_dataset, batch_size=32, shuffle=True)\n","lf_test_dataset = BaseDataset(data_path = \"Low Quality/f2f_data/test\")\n","lf_testloader = DataLoader(lf_test_dataset, batch_size=32, shuffle=True)\n","\n","lf = Experiment(\"LowQuality-Face2Face\")\n","lf.train_model(lf_trainloader, lf_valloader)\n","lf.test_model(lf_testloader)"],"metadata":{"id":"ewcP7li5CVCb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4) LowQuality-NeuralTexture\n","\n","ln_train_dataset = BaseDataset(data_path = \"Low Quality/nt_data/train\")\n","ln_trainloader = DataLoader(ln_train_dataset, batch_size=32, shuffle=True)\n","ln_val_dataset = BaseDataset(data_path = \"Low Quality/nt_data/val\")\n","ln_valloader = DataLoader(ln_val_dataset, batch_size=32, shuffle=True)\n","ln_test_dataset = BaseDataset(data_path = \"Low Quality/nt_data/test\")\n","ln_testloader = DataLoader(ln_test_dataset, batch_size=32, shuffle=True)\n","\n","ln = Experiment(\"LowQuality-NeuralTexture\")\n","ln.train_model(ln_trainloader, ln_valloader)\n","ln.test_model(ln_testloader)"],"metadata":{"id":"4ecbU6eXCn9M"},"execution_count":null,"outputs":[]}]}