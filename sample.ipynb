{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"DHSMpeFLMPTV","executionInfo":{"status":"ok","timestamp":1684128449703,"user_tz":-540,"elapsed":17757,"user":{"displayName":"문보현","userId":"17059632883058912178"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7631e8ed-534c-4ee0-d59f-7a48c0385cb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd  drive/MyDrive/Colab Notebooks/'Deepfake Asnmnt'/"],"metadata":{"id":"NYWu4SJ_MeKv","executionInfo":{"status":"ok","timestamp":1684128449703,"user_tz":-540,"elapsed":7,"user":{"displayName":"문보현","userId":"17059632883058912178"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b942cf35-0866-488b-eae9-0097965fc8cf"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Deepfake Asnmnt\n"]}]},{"cell_type":"code","source":["!pip install timm"],"metadata":{"id":"deSal94BVhai","executionInfo":{"status":"ok","timestamp":1684128456308,"user_tz":-540,"elapsed":6609,"user":{"displayName":"문보현","userId":"17059632883058912178"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dff1e170-7e1b-4ebf-b571-8df42ba56e14"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm\n","  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.1+cu118)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n","Collecting huggingface-hub (from timm)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors (from timm)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n","Installing collected packages: safetensors, huggingface-hub, timm\n","Successfully installed huggingface-hub-0.14.1 safetensors-0.3.1 timm-0.9.2\n"]}]},{"cell_type":"code","source":["import timm \n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.transforms import transforms\n","from PIL import Image\n","from sklearn.metrics import precision_recall_fscore_support"],"metadata":{"id":"_TbdxpHFNHSS","executionInfo":{"status":"ok","timestamp":1684128462890,"user_tz":-540,"elapsed":6595,"user":{"displayName":"문보현","userId":"17059632883058912178"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"0QZZuaDgMn7u","executionInfo":{"status":"ok","timestamp":1684128462890,"user_tz":-540,"elapsed":8,"user":{"displayName":"문보현","userId":"17059632883058912178"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# num_classes = 2\n","# xc_model = timm.create_model('xception', pretrained = True, num_classes = num_classes).to(device)\n","# ef_model = timm.create_model('efficientnet_b0', pretrained=True, num_classes = num_classes).to(device)\n","\n","# criterion = nn.BCEWithLogitsLoss()\n","# optimizer_xc = optim.Adam(xc_model.parameters(), lr=0.001)\n","# optimizer_ef = optim.Adam(ef_model.parameters(), lr=0.001)\n","\n","# x     = torch.randn(32, 3, 224, 224).to(device) # 32 is batch_size\n","# ef_model(x).shape"],"metadata":{"id":"mZ6nOu1lWW8V","executionInfo":{"status":"ok","timestamp":1684128462890,"user_tz":-540,"elapsed":6,"user":{"displayName":"문보현","userId":"17059632883058912178"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class BaseDataset(Dataset):\n","    def __init__(self, data_path):\n","        self.data_path = data_path\n","\n","        # normalize\n","        self.transforms = transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","        \n","        self.imgs = []\n","        self.labels = []\n","        \n","        # parsing and getting labels\n","        for file in os.listdir(data_path):\n","            if file.endswith('.png'):\n","                img_path = os.path.join(data_path, file)\n","                label = file.split('_')[0]\n","                self.imgs.append(img_path)\n","                self.labels.append(label)\n","        \n","    def __len__(self):\n","        return len(self.imgs)\n","    \n","    def __getitem__(self, idx):\n","        img = Image.open(self.imgs[idx])\n","        img = self.transforms(img)\n","        label = 1 if self.labels[idx] == 'fake' else 0\n","        return img, label"],"metadata":{"id":"Nek81guOijFo","executionInfo":{"status":"ok","timestamp":1684128462891,"user_tz":-540,"elapsed":6,"user":{"displayName":"문보현","userId":"17059632883058912178"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Experiment Class\n","class Experiment():\n","    def __init__(self):\n","        self.num_classes = 2\n","        self.xc_model = timm.create_model('xception', pretrained = True, num_classes = self.num_classes).to(device)\n","        self.ef_model = timm.create_model('efficientnet_b0', pretrained=True, num_classes = self.num_classes).to(device)\n","\n","        self.criterion = nn.BCEWithLogitsLoss()\n","        self.optimizer_xc = optim.Adam(self.xc_model.parameters(), lr=0.001, weight_decay = 0.001)\n","        self.optimizer_ef = optim.Adam(self.ef_model.parameters(), lr=0.001, weight_decay = 0.001)\n","\n","\n","    def train_model(self, trainloader, valloader):\n","        num_epochs = 100\n","\n","        xc_train_accs = []\n","        xc_val_accs = []\n","        ef_train_accs = []\n","        ef_val_accs = []\n","\n","        for epoch in range(num_epochs):\n","            # Training loop for Xception model\n","            self.xc_model.train()\n","            xc_total_loss = 0\n","            xc_total_correct = 0\n","            for images, labels in trainloader:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                self.optimizer_xc.zero_grad()\n","                outputs = self.xc_model(images)\n","                labels_onehot = F.one_hot(labels, num_classes = self.num_classes).float()\n","                loss = self.criterion(outputs, labels_onehot)\n","                loss.backward()\n","                self.optimizer_xc.step()\n","\n","                xc_total_loss += loss.item()\n","                xc_total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","\n","            xc_train_loss = xc_total_loss / len(trainloader)\n","            xc_train_acc = xc_total_correct / len(trainloader.dataset)\n","            xc_train_accs.append(xc_train_acc)\n","\n","            print(f'Epoch {epoch+1} - Xception model - Training loss: {xc_train_loss:.4f} - Training accuracy: {xc_train_acc:.4f}')\n","\n","            # Training loop for EfficientNet model\n","            self.ef_model.train()\n","            ef_total_loss = 0\n","            ef_total_correct = 0\n","            for images, labels in trainloader:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                self.optimizer_ef.zero_grad()\n","                outputs = self.ef_model(images)\n","                labels_onehot = F.one_hot(labels, num_classes=self.num_classes).float()\n","                loss = self.criterion(outputs, labels_onehot)\n","                loss.backward()\n","                self.optimizer_ef.step()\n","\n","                ef_total_loss += loss.item()\n","                ef_total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","\n","            ef_train_loss = ef_total_loss / len(trainloader)\n","            ef_train_acc = ef_total_correct / len(trainloader.dataset)\n","            ef_train_accs.append(ef_train_acc)\n","\n","            print(f'Epoch {epoch+1} - EfficientNet model - Training loss: {ef_train_loss:.4f} - Training accuracy: {ef_train_acc:.4f}')\n","\n","\n","            # Validation loop\n","            self.xc_model.eval()\n","            self.ef_model.eval()\n","            with torch.no_grad():\n","                xc_total, xc_correct = 0, 0\n","                for images, labels in valloader:\n","                    images = images.to(device)\n","                    labels = labels.to(device)\n","                    outputs = self.xc_model(images)\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    xc_total += labels.size(0)\n","                    xc_correct += (predicted == labels).sum().item()\n","                xc_accuracy = xc_correct / xc_total\n","                xc_val_accs.append(xc_accuracy)\n","            \n","                print(f'Epoch {epoch+1} - Xception model - Validation accuracy: {xc_accuracy:.4f}')\n","\n","                ef_total, ef_correct = 0, 0\n","                for images, labels in valloader:\n","                    images = images.to(device)\n","                    labels = labels.to(device)\n","                    outputs = self.ef_model(images)\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    ef_total += labels.size(0)\n","                    ef_correct += (predicted == labels).sum().item()\n","                ef_accuracy = ef_correct / ef_total\n","                ef_val_accs.append(ef_accuracy)\n","\n","                print(f'Epoch {epoch+1} - EfficientNet model - Validation accuracy: {ef_accuracy:.4f}')\n","\n","        plt.plot(xc_train_accs, label='Xception Training Accuracy')\n","        plt.plot(xc_val_accs, label='Xception Validation Accuracy')\n","        plt.plot(ef_train_accs, label='EfficientNet Training Accuracy')\n","        plt.plot(ef_val_accs, label='EfficientNet Validation Accuracy')\n","        plt.legend()\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Accuracy')\n","        plt.show()\n","\n","    def test_model(self, testloader):\n","\n","        self.xc_model.eval()\n","        self.ef_model.eval()\n","        with torch.no_grad():\n","            xc_total, xc_correct = 0, 0\n","            xc_true_labels, xc_predicted_labels = [], []\n","            for images, labels in testloader:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                outputs = self.xc_model(images)\n","                _, predicted = torch.max(outputs.data, 1)\n","                xc_total += labels.size(0)\n","                xc_correct += (predicted == labels).sum().item()\n","                xc_true_labels += labels.cpu().tolist()\n","                xc_predicted_labels += predicted.cpu().tolist()\n","            xc_accuracy = xc_correct / xc_total\n","            xc_precision, xc_recall, xc_f1, _ = precision_recall_fscore_support(xc_true_labels, xc_predicted_labels, average='binary')\n","        \n","            print(f'Xception model - Test accuracy: {xc_accuracy:.4f}')\n","            print(f'Xception model - Test precision: {xc_precision:.4f}')\n","            print(f'Xception model - Test recall: {xc_recall:.4f}')\n","            print(f'Xception model - Test F1 score: {xc_f1:.4f}')\n","\n","\n","            ef_total, ef_correct = 0, 0\n","            ef_true_labels, ef_predicted_labels = [], []\n","            for images, labels in testloader:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                outputs = self.ef_model(images)\n","                _, predicted = torch.max(outputs.data, 1)\n","                ef_total += labels.size(0)\n","                ef_correct += (predicted == labels).sum().item()\n","                ef_true_labels += labels.cpu().tolist()\n","                ef_predicted_labels += predicted.cpu().tolist()\n","            ef_accuracy = ef_correct / ef_total\n","            ef_precision, ef_recall, ef_f1, _ = precision_recall_fscore_support(ef_true_labels, ef_predicted_labels, average='binary')\n","\n","            print(f'EfficientNet model - Test accuracy: {ef_accuracy:.4f}')\n","            print(f'EfficientNet model - Test precision: {ef_precision:.4f}')\n","            print(f'EfficientNet model - Test recall: {ef_recall:.4f}')\n","            print(f'EfficientNet model - Test F1 score: {ef_f1:.4f}')"],"metadata":{"id":"V-bM1v7FhGbS","executionInfo":{"status":"ok","timestamp":1684128975929,"user_tz":-540,"elapsed":310,"user":{"displayName":"문보현","userId":"17059632883058912178"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["#1) HighQuality-Face2Face\n","\n","hf_train_dataset = BaseDataset(data_path = \"High Quality/f2f_data/train\")\n","hf_trainloader = DataLoader(hf_train_dataset, batch_size=32, shuffle=True)\n","hf_val_dataset = BaseDataset(data_path = \"High Quality/f2f_data/val\")\n","hf_valloader = DataLoader(hf_val_dataset, batch_size=32, shuffle=True)\n","hf_test_dataset = BaseDataset(data_path = \"High Quality/f2f_data/test\")\n","hf_testloader = DataLoader(hf_test_dataset, batch_size=32, shuffle=True)\n","\n","hf = Experiment()\n","hf.train_model(hf_trainloader, hf_valloader)\n","hf.test_model(hf_testloader)"],"metadata":{"id":"ZEka6VQlWX__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2) HighQuality-NeuralTexture\n","\n","hn_train_dataset = BaseDataset(data_path = \"High Quality/nt_data/train\")\n","hn_trainloader = DataLoader(hn_train_dataset, batch_size=32, shuffle=True)\n","hn_val_dataset = BaseDataset(data_path = \"High Quality/nt_data/val\")\n","hn_valloader = DataLoader(hn_val_dataset, batch_size=32, shuffle=True)\n","hn_test_dataset = BaseDataset(data_path = \"High Quality/nt_data/test\")\n","hn_testloader = DataLoader(hn_test_dataset, batch_size=32, shuffle=True)\n","\n","hn = Experiment()\n","hn.train_model(hn_trainloader, hn_valloader)\n","hn.test_model(hn_testloader)"],"metadata":{"id":"CdBRsk3R9wQf","executionInfo":{"status":"aborted","timestamp":1684128639418,"user_tz":-540,"elapsed":8,"user":{"displayName":"문보현","userId":"17059632883058912178"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3) LowQuality-Face2Face\n","\n","lf_train_dataset = BaseDataset(data_path = \"Low Quality/f2f_data/train\")\n","lf_trainloader = DataLoader(lf_train_dataset, batch_size=32, shuffle=True)\n","lf_val_dataset = BaseDataset(data_path = \"Low Quality/f2f_data/val\")\n","lf_valloader = DataLoader(lf_val_dataset, batch_size=32, shuffle=True)\n","lf_test_dataset = BaseDataset(data_path = \"Low Quality/f2f_data/test\")\n","lf_testloader = DataLoader(lf_test_dataset, batch_size=32, shuffle=True)\n","\n","lf = Experiment()\n","lf.train_model(lf_trainloader, lf_valloader)\n","lf.test_model(lf_testloader)"],"metadata":{"id":"ewcP7li5CVCb","executionInfo":{"status":"aborted","timestamp":1684128639419,"user_tz":-540,"elapsed":9,"user":{"displayName":"문보현","userId":"17059632883058912178"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4) LowQuality-NeuralTexture\n","\n","ln_train_dataset = BaseDataset(data_path = \"Low Quality/nt_data/train\")\n","ln_trainloader = DataLoader(ln_train_dataset, batch_size=32, shuffle=True)\n","ln_val_dataset = BaseDataset(data_path = \"Low Quality/nt_data/val\")\n","ln_valloader = DataLoader(ln_val_dataset, batch_size=32, shuffle=True)\n","ln_test_dataset = BaseDataset(data_path = \"Low Quality/nt_data/test\")\n","ln_testloader = DataLoader(ln_test_dataset, batch_size=32, shuffle=True)\n","\n","ln = Experiment()\n","ln.train_model(ln_trainloader, ln_valloader)\n","ln.test_model(ln_testloader)"],"metadata":{"id":"4ecbU6eXCn9M","executionInfo":{"status":"aborted","timestamp":1684128639419,"user_tz":-540,"elapsed":8,"user":{"displayName":"문보현","userId":"17059632883058912178"}}},"execution_count":null,"outputs":[]}]}